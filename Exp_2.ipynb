{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ1IkTCtoyeUVZqhYrdF1N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyaraut17/ANN/blob/main/Exp_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7sWWKHFijM_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data: x (input), y (actual output)\n",
        "x = np.array([50, 60, 70, 80, 90])  # Input features\n",
        "y = np.array([10, 12, 14, 16, 18]) # Actual outputs (for example, y = 2 * x)\n",
        "\n",
        "# Initialize weight and bias\n",
        "w = 0  # initial guess for weight\n",
        "b = 0  # initial guess for bias\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Gradient Descent\n",
        "for i in range(num_iterations):\n",
        "    y_pred = w * x + b  # Predicted output\n",
        "    error = y_pred - y  # Error (difference between predicted and true values)\n",
        "\n",
        "    # Compute gradients (derivatives of cost function)\n",
        "    dw = (2/len(x)) * np.sum(error * x)  # Gradient with respect to weight\n",
        "    db = (2/len(x)) * np.sum(error)      # Gradient with respect to bias\n",
        "\n",
        "    # Update the weights and bias\n",
        "    w = w - learning_rate * dw\n",
        "    b = b - learning_rate * db\n",
        "\n",
        "    # Optionally print progress\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Iteration {i}: w = {w:.4f}, b = {b:.4f}, MSE = {np.mean(error**2):.4f}\")\n",
        "\n",
        "# Plot the result\n",
        "plt.scatter(x, y, color='blue', label='True values')\n",
        "plt.plot(x, w * x + b, color='red', label='Fitted line')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Linear Regression with Gradient Descent')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final weight (w) = {w:.4f}, Final bias (b) = {b:.4f}\")"
      ]
    }
  ]
}