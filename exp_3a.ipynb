{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAwt2jui0rcjQWzcDNwH5G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyaraut17/ANN/blob/main/exp_3a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY3vmKjutsty",
        "outputId": "67e00b30-55be-46d7-b481-436a4a09665f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study Hours= [ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
            "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
            "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
            "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
            "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
            "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
            "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
            "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
            "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
            "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
            "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
            "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
            "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
            "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
            "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
            "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
            "  9.72727273  9.81818182  9.90909091 10.        ]\n",
            "Score=\n",
            " [ 8.52810469  6.25485987  7.86656688 10.84542276 10.5532978   5.31817151\n",
            "  9.62744956  7.87910377  8.42992593  9.91210609  9.83354169 12.90854701\n",
            " 11.9766209  11.15244094 12.25136283 12.48553047 15.26088542 12.3169562\n",
            " 13.80795359 11.92817216  8.98492946 15.85269174 16.7288724  13.97021541\n",
            " 20.44860016 13.45490501 16.90969885 16.89835957 20.79283116 21.12053572\n",
            " 18.94625849 19.84723413 17.76988305 16.03840706 19.75872116 21.22178885\n",
            " 23.82421773 24.22294152 21.49807364 22.12266723 21.08471225 20.79632776\n",
            " 20.67836871 28.44700534 23.98069564 24.57839685 23.40350019 27.91861708\n",
            " 23.59038612 26.84724671 25.9363396  28.95562318 27.61475336 26.72964472\n",
            " 29.48909009 30.85666374 30.5875799  31.5140347  30.09499218 31.09269949\n",
            " 30.92780638 32.0081664  31.55552562 30.18379843 34.44576138 33.74189267\n",
            " 31.73960331 36.38010997 34.09449418 36.46752716 38.27636294 37.53069309\n",
            " 40.0060741  35.71216654 39.44104692 37.72128891 37.80386025 38.84230067\n",
            " 39.83144039 41.02142159 39.03333668 43.61983479 43.20405215 39.65478535\n",
            " 46.15832257 47.42814199 46.44846823 44.18560487 42.85849476 47.56344891\n",
            " 45.10273702 48.8085265  47.23473177 49.22600535 48.44000552 49.59496452\n",
            " 48.65736368 52.66265008 49.79927873 50.80397873]\n",
            "Normalized Study Hours: ['-1.7148', '-1.6802', '-1.6455', '-1.6109', '-1.5762', '-1.5416', '-1.5070', '-1.4723', '-1.4377', '-1.4030', '-1.3684', '-1.3337', '-1.2991', '-1.2645', '-1.2298', '-1.1952', '-1.1605', '-1.1259', '-1.0912', '-1.0566', '-1.0220', '-0.9873', '-0.9527', '-0.9180', '-0.8834', '-0.8487', '-0.8141', '-0.7795', '-0.7448', '-0.7102', '-0.6755', '-0.6409', '-0.6062', '-0.5716', '-0.5370', '-0.5023', '-0.4677', '-0.4330', '-0.3984', '-0.3637', '-0.3291', '-0.2945', '-0.2598', '-0.2252', '-0.1905', '-0.1559', '-0.1212', '-0.0866', '-0.0520', '-0.0173', '0.0173', '0.0520', '0.0866', '0.1212', '0.1559', '0.1905', '0.2252', '0.2598', '0.2945', '0.3291', '0.3637', '0.3984', '0.4330', '0.4677', '0.5023', '0.5370', '0.5716', '0.6062', '0.6409', '0.6755', '0.7102', '0.7448', '0.7795', '0.8141', '0.8487', '0.8834', '0.9180', '0.9527', '0.9873', '1.0220', '1.0566', '1.0912', '1.1259', '1.1605', '1.1952', '1.2298', '1.2645', '1.2991', '1.3337', '1.3684', '1.4030', '1.4377', '1.4723', '1.5070', '1.5416', '1.5762', '1.6109', '1.6455', '1.6802', '1.7148']\n",
            "w= 1.8831506970562544\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def Sigmoid(x):\n",
        "    return 1 / (1+np.exp(-x))\n",
        "def ReLu(x):\n",
        "    return np.maximum(0,x)\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "np.random.seed(0)\n",
        "study_hrs=np.linspace(1,10,100)\n",
        "# print(\"Study Hours=\",student)\n",
        "print(\"Study Hours=\",study_hrs)\n",
        "\n",
        "score=5*study_hrs+np.random.normal(0,2,100)\n",
        "print(\"Score=\\n\",score)\n",
        "\n",
        "study_hrs=(study_hrs-np.mean(study_hrs))/np.std(study_hrs)\n",
        "print(\"Normalized Study Hours:\",[f\"{x:.4f}\" for x in (study_hrs)])\n",
        "\n",
        "\n",
        "w=np.random.randn()\n",
        "print(\"w=\",w)\n",
        "b=np.random.randn()\n",
        "lr=0.0001\n",
        "def grad_desc(act_fun,epochs=1750):\n",
        "    global w,b\n",
        "    for epoch in range(epochs):\n",
        "        prediction=act_fun(w*study_hrs+b)\n",
        "        error=prediction-score\n",
        "        dw=np.mean(error*study_hrs)\n",
        "        db=np.mean(error)\n",
        "        w=w-lr*dw\n",
        "        b=b-lr*db\n",
        "        if(epoch%200==0):\n",
        "            cost=np.mean(error**2)\n",
        "            print(\"weight & bias\",w,b)\n",
        "    return w,b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define activation functions\n",
        "def Sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Generate dataset\n",
        "np.random.seed(0)\n",
        "study_hrs = np.linspace(1, 10, 100)\n",
        "score = 5 * study_hrs + np.random.normal(0, 2, 100)\n",
        "\n",
        "# Normalize input data\n",
        "study_hrs = (study_hrs - np.mean(study_hrs)) / np.std(study_hrs)\n",
        "\n",
        "# Define the neural network function\n",
        "def single_layer_nn(activation_function, epochs=1000, learning_rate=0.01):\n",
        "    w = np.random.randn()\n",
        "    b = np.random.randn()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        prediction = activation_function(w * study_hrs + b)\n",
        "        error = prediction - score\n",
        "        dw = np.mean(error * study_hrs)\n",
        "        db = np.mean(error)\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "    return w, b\n",
        "# Train with different activation functions\n",
        "w_sigmoid, b_sigmoid = single_layer_nn(Sigmoid)\n",
        "w_relu, b_relu = single_layer_nn(ReLu)\n",
        "w_tanh, b_tanh = single_layer_nn(tanh)\n",
        "\n",
        "# Print the trained weights and biases for each activation function\n",
        "print(\"Sigmoid: w =\", w_sigmoid, \", b =\", b_sigmoid)\n",
        "print(\"ReLU: w =\", w_relu, \", b =\", b_relu)\n",
        "print(\"Tanh: w =\", w_tanh, \", b =\", b_tanh)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlwG1QBgD3rl",
        "outputId": "1b36c6a5-70e8-4dc6-cdef-ba8a6ba0bce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid: w = 131.1955254560606 , b = 264.9972645972221\n",
            "ReLU: w = 12.946939932241015 , b = 27.61846540017248\n",
            "Tanh: w = 128.30695317122297 , b = 268.14362626587877\n"
          ]
        }
      ]
    }
  ]
}